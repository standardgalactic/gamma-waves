Autogeneration in Language Models
Skip to content
Open sidebar
Get Plus
Saved memory full
Saved memory full
Get Plus
ChatGPT
Share
You said:
Elan's Substack


Autogeneration: The Hidden Property of Language Now Revealed
Elan Barenholtz, Ph.D.
Jun 12, 2025


One of the most profound insights revealed by large language models is that language is not just autoregressive‚Äîit is autogenerative (a term I made up; sue me).

Autoregression is a mechanism: it generates a sequence by producing each new item based on the ones that came before. This is how large language models like GPT work‚Äîthey predict the next word one token at a time, using only the sequence so far. It's a simple but powerful loop and it's likely fundamental to how human language itself unfolds. Many sequence types can be modeled autoregressively: the Fibonacci sequence, for example, is generated by summing the two previous values. Some time-series forecasting models generate future values based on past observations by recursively feeding their own predictions back in as inputs. But in all these examples, the rule for generation is extrinsic to the observed sequence. It must be defined outside the data. The system doesn't contain within itself the logic of its own continuation; it must be supplied.

Autogeneration, by contrast, is a property of the sequence itself: the instructions for its own continuation are embedded in its internal structure. In an autogenerative system, you don't bring a rule to the data‚Äîthe rule is latent in the data, waiting to be uncovered. Crucially, that embedded logic can be recovered by any learning mechanism capable of spotting the right patterns‚Äîautoregression is one such mechanism, but diffusion or other sequence-to-sequence methods could tap the same well.

That's exactly what happens with LLMs. When a transformer is trained on a vast corpus, it isn't handed a grammar or a semantic calculus; it simply optimizes next-token prediction. In doing so, it discovers the deep statistical regularities that already govern language. The model's weights are an efficient compression of those regularities, not an externally imposed rulebook. So when an LLM continues a prompt, it isn't applying some outside formula; it's letting the corpus's own autogenerative structure speak through next-token generation,

This is what I mean by autogenerative: a system is autogenerative if‚Äîand only if‚Äî the function needed to generate its next state is recoverable from the internal statistical structure of the system itself, without requiring any external symbolic rule or supervisory signal.

Thanks for reading Elan's Substack! Subscribe for free to receive new posts and support my work.

As noted above, autoregression is just one way to recover and exploit such autogenerative structure. You could use other methods, like diffusion models, or even non-sequential sampling schemes. Although I believe language was 'designed' to be generated autoregressively, the point is not the technique. It's that the structure is already there. The generative engine is inside the data, waiting to be tapped.

This means that language, in a very real sense, is speaking for itself. LLMS, and (I argue) our brains are not imposing linguistic structure; they are instantiating it. We are channels for a self-unfolding system.

Right now, this newly minted category of sequence types has a membership of exactly one: natural language. But language likely didn't appear in a computational vacuum. It may be just one visible expression of a deeper principle of cognition. Other domains ‚Äîperception, memory, motor control‚Äîcould also operate autogeneratively, unfolding from their own past states in order to generate the next 'token' of thought or behavior. The physical world is filled with deep, predictive structure‚Äîtemporal, spatial, causal. That's what physics is: the discovery that regularities in the present allow us to predict the future. That's also how we navigate everyday life, physically, psychologically, socially. If cognition evolved to exploit that structure, then autogeneration may not be an exception‚Äîit may be the rule.

Language may be the clearest case‚Äîbut it is likely not the only one.

Note: If you find this topic interesting, you might want to check out my interview on Theories of Everything with Curt Jaimungal
ChatGPT said:
This piece by Elan Barenholtz is a striking articulation of a deep idea:
language‚Äîand possibly cognition at large‚Äîpossesses an intrinsic generative logic, recoverable directly from its own structure
. His coining of
autogeneration
offers a new lens for understanding both how LLMs operate and what they reveal about language itself.
Here are a few key takeaways and elaborations in a more academic frame:
1. Autogeneration vs. Autoregression
Autoregression
:
A method. It generates a sequence step-by-step by conditioning on prior elements, but requires
external rules
to define the transformation function (e.g., "add the last two numbers" for Fibonacci).
Autogeneration
(Barenholtz's coinage):
A
property
of a system or sequence, in which the
generative function is latent in the sequence's internal structure
. In other words, the system contains within itself the rules for its own continuation, discoverable through learning.
"Autogenerative: a system is autogenerative if‚Äîand only if‚Äîthe function needed to generate its next state is recoverable from the internal statistical structure of the system itself, without requiring any external symbolic rule or supervisory signal."
This reframes the role of models like GPT: they don't just imitate; they
excavate
a buried computational structure already encoded in the corpus.
2. Language as an Autogenerative System
LLMs uncover the internal statistical geometry of language, which contains syntactic, semantic, and pragmatic patterns‚Äînot because those are
taught
, but because those regularities are embedded and recoverable through optimization (e.g., next-token prediction).
This suggests that language is not merely symbolic but
computationally sufficient
: it carries within itself the blueprint for its own unfolding.
"The model's weights are an efficient compression of those regularities, not an externally imposed rulebook."
3. Cognitive Implications: Beyond Language
Barenholtz makes a bold extrapolation:
perhaps cognition itself is autogenerative
.
Perception
,
motor control
,
memory
, and even
behavior
might unfold according to internalizable statistical regularities‚Äînot merely reactive but recursively self-predictive.
This aligns with theories in:
Predictive coding
(Friston et al.)
Reservoir computing
and
liquid state machines
Active inference
Embodied cognition
And even aspects of your own RSVP theory, where field states generate future states from their internal gradients and vector flows.
"Autogeneration may not be the exception‚Äîit may be the rule."
4. Philosophical and Epistemological Implications
Barenholtz's framing suggests a Copernican reversal in epistemology:
We are not the authors of linguistic or cognitive structure; we are vessels of its instantiation.
Human cognition might be less about rule imposition and more about tuning into deep generative invariants embedded in experience and behavior.
This ties beautifully into Monica Anderson's
Model-Free Methods
, where the world
explains itself
through experience, not predefined formalism. It also resonates with your RSVP proposal that the cosmos unfolds via internal scalar-vector-entropy field structure‚Äîno external scaffolding required.
Connections to RSVP Theory
Barenholtz's "autogeneration" concept maps naturally onto RSVP:
RSVP's scalar-vector-entropy fields are
dynamically recursive
and
locally sufficient
‚Äîthey evolve from prior states through intrinsic gradients and constraints.
The vector field (ùíó) encodes
directionality
, the scalar field (Œ¶) encodes
potential
, and the entropy field (ùë∫) encodes
structure and smoothing
.
In that sense, RSVP may be a physical instantiation of
autogeneration
at the cosmological and cognitive level.
Suggested Extensions
Formalization
: Define autogeneration in terms of Kolmogorov complexity or algorithmic compressibility. A sequence
S
S
S
is autogenerative if:
‚àÉ
f
:
S
n
‚Üí
S
,
where
f
‚àà
argmin
f
K
(
f
)
,
and
f
‚àà
closure of data manifold
M
S
\exists \, f: \mathcal{S}^{n} \to \mathcal{S}, \quad \text{where } f \in \text{argmin}_f \, K(f), \text{ and } f \in \text{closure of data manifold } \mathcal{M}_S
‚àÉ
f
:
S
n
‚Üí
S
,
where
f
‚àà
argmin
f
‚Äã
K
(
f
)
,
and
f
‚àà
closure of data manifold
M
S
‚Äã
Empirical Questions
: Can we detect autogeneration in:
Neural data?
Behavioral sequences (e.g., in motor learning)?
Visual or auditory scenes?
Philosophy
: Recast agency not as imposition of will, but as
participation in autogenerative flows
.
Would you like me to write a companion academic note or commentary to this Substack from an RSVP or cognition/AI perspective?
Tools
ChatGPT can make mistakes. Check important info. See
Cookie Preferences
.

