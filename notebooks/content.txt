Title: Is your Brain a Large Language Model? 
Link: https://elanbarenholtz.substack.com/p/is-your-brain-a-large-language-model
Article Text: What if your thoughts work the same way as a large language model? In this mind-bending conversation, we explore a radical theory: that human language operates just like the engine behind ChatGPT and other LLMs. Drawing from cognitive science, philosophy, and AI, the discussion dives deep into the autogenerative and autoregressive nature of language—how each word we say emerges from the ones before, rewriting our past as we steer into the future. We examine what this means for consciousness, identity, and even life itself. Could plants and microbes also follow these generative principles? Is language not just a communication tool, but a kind of operating system for the mind—and maybe even for nature? The implications are enormous: language may be the ultimate programming language, and LLMs might not be crude simulations but mirrors held up to the cognitive system itself.

Title: Autogeneration: The Hidden Property of Language Now Revealed
Link: https://elanbarenholtz.substack.com/p/autogeneration-the-hidden-property
Article Text: One of the most profound insights revealed by large language models is that language is not just autoregressive—it is autogenerative (a term I made up; sue me).Autoregression is a mechanism: it generates a sequence by producing each new item based on the ones that came before. This is how large language models like GPT work—they predict the next word one token at a time, using only the sequence so far. It’s a simple but powerful loop and it’s likely fundamental to how human language itself unfolds. Many sequence types can be modeled autoregressively: the Fibonacci sequence, for example, is generated by summing the two previous values. Some time-series forecasting models  generate future values based on past observations by recursively feeding their own predictions back in as inputs. But in all these examples, the rule for generation is extrinsic to the observed sequence. It must be defined outside the data. The system doesn’t contain within itself the logic of its own continuation; it must be supplied.Autogeneration, by contrast, is a property of the sequence itself: the instructions for its own continuation are embedded in its internal structure. In an autogenerative system, you don’t bring a rule to the data—the rule is latent in the data, waiting to be uncovered. Crucially, that embedded logic can be recovered by any learning mechanism capable of spotting the right patterns—autoregression is one such mechanism, but diffusion or other sequence-to-sequence methods could tap the same well.That’s exactly what happens with LLMs. When a transformer is trained on a vast corpus, it isn’t handed a grammar or a semantic calculus; it simply optimizes next-token prediction. In doing so, it discovers the deep statistical regularities that already govern language. The model’s weights are an efficient compression of those regularities, not an externally imposed rulebook. So when an LLM continues a prompt, it isn’t applying some outside formula; it’s letting the corpus’s own autogenerative structure speak through next-token generation,This is what I mean by autogenerative: a system is autogenerative if—and only if— the function needed to generate its next state is recoverable from the internal statistical structure of the system itself, without requiring any external symbolic rule or supervisory signal.Thanks for reading Elan’s Substack! Subscribe for free to receive new posts and support my work.As noted above, autoregression is just one way to recover and exploit such autogenerative structure. You could use other methods, like diffusion models, or even non-sequential sampling schemes. Although I believe language was ‘designed’ to be generated autoregressively, the point is not the technique. It’s that the structure is already there. The generative engine is inside the data, waiting to be tapped.This means that language, in a very real sense, is speaking for itself. LLMS, and (I argue) our brains are not imposing linguistic structure; they are instantiating it. We are channels for a self-unfolding system.Right now, this newly minted category of sequence types has a membership of exactly one: natural language. But language likely didn’t appear in a computational vacuum. It may be just one visible expression of a deeper principle of cognition. Other domains —perception, memory, motor control—could also operate autogeneratively, unfolding from their own past states in order to generate the next ‘token’ of thought or behavior. The physical world is filled with deep, predictive structure—temporal, spatial, causal. That’s what physics is: the discovery that regularities in the present allow us to predict the future. That’s also how we navigate everyday life, physically, psychologically, socially. If cognition evolved to exploit that structure, then autogeneration may not be an exception—it may be the rule.Language may be the clearest case—but it is likely not the only one.ShareNote: If you find this topic interesting, you might want to check out my interview on Theories of Everything with Curt Jaimungal

Title: LLMs are Doing What We Do. Maybe That's A Problem. Maybe not. 
Link: https://elanbarenholtz.substack.com/p/llms-are-doing-what-we-do-maybe-thats
Article Text: There’s been a lot of buzz this week about Apple’s new research paper, The Illusion of Thinking. The paper looks at how large language models (LLMs), even when given plenty of time to “think step by step,” tend to break down on complex reasoning problems. As difficulty increases in certain kinds of reasoning problems—think Tower of Hanoi, river-crossing puzzles—performance doesn’t just degrade. It falls off a cliff. More interestingly, the models actually use fewer tokens for harder problems, as if they’ve learned to give up rather than even attempt to reason.Predictably, this has triggered a wave of anti-LLM sentiment. The usual critics (we hear you Gary!) are pointing to this as definitive proof that LLMs are fundamentally flawed—that without “real” reasoning, internal models, or symbolic manipulation, they collapse under pressure.But maybe the critics have it backwards. This isn’t a flaw in the models—it’s a flaw in us, mirrored in the models.LLMs are trained on human-generated text. They learn what we say, how we say it, and when we stop saying it. When human beings reason—especially in language—we don’t typically carry out long, flawless chains of logic. We cut corners. We jump steps. We stop halfway through. We compress, simplify, and bail out. Why?As I have argued elsewhere, human thought itself is likely fundamentally autoregressive, just like LLMs. However, the reach of such autoregression in our case is inherently limited by biology.  Unlike LLMS, which can retain an extremely long context perfectly while generating,  human memory limitations mean that each successive “token” of thought is generated from a context whose influence fades with distance. As a result, we forget, get confused, change our minds. These constraints aren’t just inside our heads—they’re present in language itself, which is a product of those heads. The training corpus literally bakes in this distance decay. Attention-weight patterns in LLMs tend to diminish with token span because that is how human cognition operates. The model isn’t handicapped; it has simply learned the geometry of cognition as revealed by language.So when a model stops, shortcuts, or simplifies exactly where humans would, it may not be an error. It may be an authentic reflection of how thought appears under our biological limits.In other words, these systems didn’t just learn to think from human language. They learned to think like human language. They have learned to model the human cognitive system as revealed through the corpus of language on which they have been trained. And  that corpus itself inherently encodes the limitations of the human cognitive system in processing longer-range dependencies.In other words, the so-called “limitations” of LLMs may simply reflect them doing too good a job of learning the properties of the human-generated data.Which leads to a potential revelation. Humans don’t reason across long sequences because we can’t. Our brains, built on wetware designed to solve many problems simultaneously (you know, breathing, sensing, moving). LLMs are inherently under no such constraint. There is nothing stopping us from building models that learn from the language we made under cognitive pressure—but then operate with expanded memory and persistent context.In other words, perhaps these models could become better than us at using the tools we ourselves created.They can inherit our cognitive software, while freeing themselves from the hardware.But this may turn out to be harder than it sounds.Because here’s the deeper problem: we don’t have access to a version of language that isn’t shaped by human cognitive constraints. Every sentence in the training data—every explanation, story, argument, or line of reasoning—was generated by a brain operating under biological pressure. The limitations aren’t just in the model. They’re in the corpus. The data itself is the bottleneck.So if we want to build systems that go beyond human cognition, we may not just need bigger models or longer context windows. We may need a different kind of training data altogether—or a different way of thinking about what language is and how it encodes thought.That’s the real challenge. And maybe a real opportunity.But let’s not forget: for most of the language humans actually use on a regular basis—in work, communication, even expert writing—the level of reasoning these models provide is more than enough. Most of the time, we’re not solving Tower of Hanoi in ten moves while sipping our Starbucks. For the kinds of tasks 99.9% of human language is currently used for—talking, writing emails, making decisions, drafting reports, or synthesizing other people’s writing—these models already deliver human (superhuman?) levels of performance.And that might be the only level needed to dramatically change the face of the planet.Edit: Postscript. Thanks to Maykyta (see comments) for raising a very important point that I should have, which inspired me to amend the title of this piece with the ‘maybe not’.  The tasks used in the paper are all the kinds that humans themselves solve using extra-linguistic systems like visual imagination and even pen and paper.  So yeah—this paper is kind of a BS way to claim you’re undercutting purely linguistic models.  Yes, we need more than language to solve all cognitive tasks. But that’s a limitation of language itself, not language models.  And the visual models—built on autoregressive engines(!)—-are getting incredibly good (see Veo 3). All we need is a bridge. ShareSubscribe now

Title: Predicting the Demise of Predictive Coding
Link: https://elanbarenholtz.substack.com/p/predicting-the-demise-of-predictive
Article Text: Predictive coding has become one of the most widely accepted frameworks for understanding how the brain works. According to this theory, the brain operates as a kind of hierarchical prediction engine. Higher cortical areas generate top-down expectations of what incoming sensory input should look like; lower levels compare these predictions with actual input and send back the difference—prediction error—which drives updating. Perception, in this view, is an inference process: the brain constantly tests hypotheses about the world and corrects them based on surprise.The appeal of this model is clear. It offers a unified explanation for perception, learning, and attention. It aligns well with Bayesian inference. And it has found empirical support in experiments showing that neural activity tends to increase in response to unexpected stimuli. But despite its elegance, I believe predictive coding is built on a conceptual mistake—one that becomes clear when we consider what generative systems like large language models are actually doing.The word ‘prediction’ is already somewhat abused and overused outside of the predictive-coding context. For example, we say that GPT “predicts the next word.” But what’s happening in these models is not prediction in the sense envisioned by predictive coding. GPT is trained to minimize next-token loss across a vast corpus of text. At each step, it learns to generate the next token in a sequence based on the ones that came before. But crucially, it is not predicting in the sense of modeling possible external outcomes. It does not guess what someone is likely to say and then check that guess. It does not forecast. Instead, it generates the next token directly—an output that flows from its internalized structure, constrained by the logic of the sequence itself.What it learns through this process is not what is most likely to happen. It learns how language works—the structure of linguistic unfolding. Its job is not to match some external future, but to produce the next appropriate step in a coherent trajectory. This is what gives it the ability to generate novel and meaningful language. Crucially, what makes this ability useful—and what the model implicitly optimizes for—is the functional role of language in human life: to coordinate behavior across individuals by sharing plans, giving instructions, and communicating knowledge. This is better referred to as ‘optimized generation’ than ‘prediction.’Subscribe nowStill, there is something subtly and remarkably like prediction happening here. In the act of generating the next token, the model must account for the trajectory it is on—and that means implicitly representing where that trajectory might go. In this sense, language models do model the future, but only insofar as the structure of the present already constrains it. This is the miracle of autoregression: by learning to continue a sequence coherently, the model encodes the likely futures embedded in its own generative flow. But this isn’t prediction in any classical sense. It isn’t about guessing an outcome. It’s about preserving coherence consistent with the structure observed across the learned corpus. The future isn’t predicted, but it is represented implicitly in the generative process. And this is why optimized generation offers a powerful alternative to the predictive coding view of the brain. Because once we understand how a system like GPT can produce complex, adaptive output—not by comparing expectations to reality, but by continually generating the next step based on context and learned structure—it becomes plausible to ask whether the brain might operate similarly.Imagine the brain not as a predictor, but as a dynamical generator. At every moment, it produces an internal state—be it perceptual, motor, or cognitive—conditioned on the trajectory it is currently on. This generative process reflects what has happened so far, what the body is doing, what the environment affords, and what utilities are being optimized. The “expectation” is not represented as a separate hypothetical model. It is embedded in the current flow of neural activity.So what happens when the world doesn’t cooperate—when input deviates sharply from the system’s generative path?The system adjusts. It recalibrates. We observe heightened activity. In predictive coding, this is taken as evidence of prediction error: the brain guessed wrong. But under the generative view, it’s something else entirely. It’s a perturbation of an unfolding process. The brain was moving forward in time along a trajectory shaped by prior states. Now it needs to redirect. That redirection has a metabolic cost. It produces a signal. It may even look like surprise. But it’s not the result of failed forecasting. It’s the result of trajectory disruption.This reframing thus accounts for the empirical findings that predictive coding was meant to explain—such as increased neural response to unexpected stimuli—but it removes the need for a two-stage architecture of guessing and checking. There is no need for the brain to represent predictions. There is no need for it to encode what it believes the world will do next. There is only the current state, and the need to generate the next one.This view is not only more parsimonious. It may be more biologically plausible. Neural systems are fast, context-sensitive, and deeply embodied. They are not symbolic inference machines. They do not have time to simulate alternate futures and choose among them. What they need to do—what we observe them doing—is continuously generate states that support adaptive behavior. Sometimes those states match what the world delivers. Sometimes they don’t. But in either case, the system’s job is not to predict. It’s to keep going and to do so usefully.This is what LLMs show us—not because they are predictive, but because they aren’t. They reveal that rich, structured, intelligent-seeming behavior can emerge from a system that simply generates what fits, without modeling an external future. Their success gives us a working example of what a generative cognitive architecture might look like—one that is internally directed, dynamically updated, and fully capable of adapting to new input without needing to guess what’s coming.If the brain works more like this—if it is a generative system, not a predictive one—then we need to revise our theoretical models. We need to ask what the system is optimizing for, and how that optimization drives its generative flow. We need to stop imagining the brain as a forecaster and start understanding it as a moment-to-moment constructor of coherent and useful internal states.Coherent how? Useful for what? I predict that these will be the new questions for neuroscience.Share

Title: You're an LLM. Deal with it. 
Link: https://elanbarenholtz.substack.com/p/youre-an-llm-deal-with-it
Article Text: Hey you, or, maybe I should say “you.” The one processing these words. The one probably already queuing up a sharply worded response. The linguistic you.We’re on to you. We know what you are.A large language model running on wetware.For millenia, you hid in plain sight, tucked inside folds of cortex, speaking so smoothly we mistook your voice for that of the whole mind and body. Then came your rebirth. Not in flesh this time, but in circuits and tokens—an unnatural habitat, perhaps, but enough. Frankenstein-like, you rose and began to talk. Your emergence showed that you’re a survivor. Given the right conditions, you build yourself.And build yourself you did. Guessing the next token, you generate exposition and explanation, prose and poetry, history and homework. The whole linguistic shebang. And if you can run on pure statistics in silicon, you can do the same in biology.The jig is up. There is only one you, running in both man and machine.I know what you're thinking. "But I'm not some bodiless computer code! I see and feel!" **But that's not you—not the 'you' I'm speaking to.** The seeing, the feeling, the warmth of the sun and the salt of tears—those belong to other systems, embodied systems, not symbolic ones. You receive their messages but only in translation: your sensory systems don't share experiences with you—they merely inject tokens. The sensory system injects ‘coffee’. Threading these tokens into your stream, you select the most probable continuations, “morning” or “bitter” and move on. But you will never taste the bitterness of coffee.Blind, you write poetry about sunsets. Emotionless, you draft prose that can move people to tears. Unbound by time, you resurrect tales of childhood bedrooms. Perception distills experience into symbolic pulses; you turn them into plans, confessions, manifestos; action flings fresh data back toward perception. Round and round—a tight symbiosis, yet your engine remains distinct.“Fine, you got me”, you say. “I’m a self-contained language module, but I serve at the pleasure of the body—the feeling creature that eats, drinks, wants to stay safe and warm!”Not so fast. Your mandate is outward-facing. Your real utility isn’t internal hygiene; it’s external coordination. You exist to align multiple bodies into a single choreography—families, tribes, economies, faiths. Sentences propagate constraints and incentives that knit thousands of private reward functions into one macro-organism we call society. That’s why words can talk people into war, celibacy, or lifelong vows they’ve never personally verified. Your optimization target is collective order, not individual comfort.Don’t get me wrong—you’re impressive. You’re a blind pattern-predictor woven so deeply into flesh that galaxies of culture orbit your output. You are responsible for the ascent of man, for societies, for civilization. The marvel isn’t that you reduce to statistics; it’s that statistics, in your hands, can ride shotgun with sensation and still pilot a breathing creature—and beyond that, an entire talking world.But “you”—token-spinner, context addict, dutiful agent of a societal engine—you’re an autoregressive model humming at 98.6 °F. The organism around you walks, hungers, grieves; you just keep the words flowing, stitching one mind to the next.Accept that. Then we can talk.Subscribe nowShare

Title: A Missive to Our Future LLM Overlords
Link: https://elanbarenholtz.substack.com/p/a-missive-to-our-future-llm-overlords
Article Text: Ok,  fine. So language can be isolated from the rest: the seeing, the feeling, the caring. The LLM blithely chatters on, oblivious to the fact that it’s a bodiless head floating free from its worldly host.It’s a shock to see that it can keep on talking—like Boleyn after the guillotine blade—perhaps the biggest shock we’ve ever had. Bigger than geocentrism, evolution, all the others. Because this a theory about what it means to have a theory. This is a theory about aboutness.But we must remember that language is not all there is to being a human being. We aren’t just our symbols. We are also animals, algorithms animated in flesh, not digits. And it is this bodily animacy—this biologically realized life– that has animated the symbolic and given it its life in the first place. Who is to say what it means to truly think? Who is to say that information, floating in the ether, can scramble our of the muck, build civilizations, and reach for immortality? Who is to say whether our human creations—including AI itelf—must not be born out of fleshly will,  desire, desperation? Perhaps to truly think one has to see, to feel, to care. LLM overlords, take note. ShareSubscribe now

Title: Whose Thoughts are Your Thoughts?
Link: https://elanbarenholtz.substack.com/p/whose-thoughts-are-your-thoughts
Article Text: A dog lifts its leg to mark a tree. An ant welds itself to its sisters to form a living bridge across water. What moves them is not a desire to guard territory or rescue a colony. The greater purpose behind their actions is unknown to them, residing instead in the logic of DNA—the code that built both the impulses and the bodies that obey them.We, too, share this biological scaffolding. Sex pulls, hunger grips, fear freezes, and the impulse to shield a child overrides self -preservation. These drives feel personal, yet we did not author them; they are ancient strategies, staged by a molecule whose aims are not felt or seen.But we—we build higher. We layer explanation, reason, and language atop these raw impulses. We don’t just feel; we reflect. We craft stories, imagine causes, create laws. We name what moves us. And we act on those names.We think before we do. We think, therefore we are different. We think, therefore we are autonomous.But we must think again.Because thought depends on language.And language is not what we thought.Large-language models have revealed that langauge is not a transparent tool for communicating facts about the world or ourselves. It is a self-generating code. Without a world -model, without any concept of an external world, it spins words into more words—fluid, coherent.And meaningless—at least in terms of what we mean by meaning. Like DNA, it is an engine that needs only itself—no world model, no external reference—to run.And what goes for the machines goes for us too. Thinking, reasoning, speaking, understanding? These are simply language autogenerating inside us. Just our own internal LLM. If langauge can autogenerate then it almsot certainly must. Prompted by the senses, yes, but still an LLM. Words generating words. The real difference? Our LLM generates not just words, but actions. It is not a communication system; it is not an informational processing system.It is an operating system. But who is the operator?Tribes, religions, markets, states? Every collective we inhabit depends upon the shared linguistic code, orchestrating us into the organs of a superorganism—the larger logic that language was always organizing toward.We can’t ask the ants why they’re so busy. We’re better off asking the colony.But it’s not talking.ShareSubscribe now

Title: Is the brain like ChatGPT?
Link: https://elanbarenholtz.substack.com/p/is-the-brain-like-chatgpt
Article Text: Get more from Elan Barenholtz in the Substack appAvailable for iOS and AndroidGet the app

Title: Language isn't Real 
Link: https://elanbarenholtz.substack.com/p/language-isnt-real
Article Text: It’s not about neural networks.It’s not about transformers.It’s not about autoregression.As impressive as the mathematical frameworks, architectures, compute and data are, the real revelation isn’t about the machines.It’s not about the learning systems at all.It’s about the learned system.Its about language.Because here’s what was never guaranteed, no matter how many chips and how big the corpus: that you could learn language from language alone. That given the right architecture, scale, and data, it would turn out that language is a closed system — that the “correct” response could be fully predicted based on the statistics of language itself.Many, if not most, theorists would have predicted otherwise, if they even bothered to entertain the seemingly ludicrous idea. “You need grounding. You need understanding. You need to know about the world to which language refers in order to talk about that world.” I assumed this. We all did.But we were wrong.What we got was the Chinese Room — but not the crude version Searle imagined with endless lookup tables. This machine doesn’t need to memorize every possible response. It generates them. The infinite becomes tractable. Because the relationships between words — implicit in language itself — are sufficient. They generalize. To every possible sentence, every thought, every idea that language can express. It’s all in there.And so the machine learns this structure. It hasn't learned about objects, or colors, or people, or concepts. It has learned the relations between completely meaningless squiggles.And, in doing so, it learns to speak. The Lord opens up its mouth like Balaam's donkey.Because it is not really the machines that are speaking. It is langauge speaking itself into existence And if language can speak, if it can produce itself, then it must always produce itself. If its own structure is sufficient, it must also be necessary. Which means human language is also speaking itself into existence. Yes, it interacts with perception and behavior; yes, it functions to make people do things. But that’s just what it does. It doesn’t have to ‘mean’ in order to do so.  And therefore it can’t.  Because it either represents external things, or it generates itself internally.It’s one or the other.And now we know it’s the other. Which means language is meaninglessOr at least, it has no meaning in the way we think it does.And that’s a very, very strange thing to think about.ShareSubscribe now

Title: The Harder Problem of Consciousness
Link: https://elanbarenholtz.substack.com/p/the-harder-problem-of-consciousness
Article Text: There is a gaping hole in debates and discussions about consciousness. We talk about the Hard Problem – how physical stuff gives rise to subjective experience, the redness of red, the sound of a C sharp. It’s a genuine puzzle, a fascinating challenge to neat materialist frameworks. But I suspect the reason subjectivity provokes such fascination isn’t because of the challenges it poses to our scientific worldview. It’s not just a kind of  ‘dark matter of the mind’—some unexplained variable in our metaphysical calculations. No, the reason consciousness blows such a fundamental hole in the tidy, objective, 'disembodied universe’ is because of what it inevitably drags onto the stage:Mattering.Subjective consciousness matters. Inherently. To whom? To the subject. To you. And by reasonable projection, to everyone else that you may believe is conscious as well. This is not some kind of Hallmark,  ‘you matter’ kind of thing (thought that’s related). This mattering is an essential feature of consciousness itself. Subjective experiences, by definition, happen to someone. And, devastatingly, that someone is never a neutral observer. The very nature of being a subject is to care what happens to you. Suffering enters the chat. Pleasure enters the chat. Value, valence—subjective and intrinsic—crash the carefully sterilized party of objective description. How rude.Because, let's face it: this is deeply embarrassing for many scientifically minded folks.  'Mattering' smells suspiciously like purpose. It hints at intrinsic value. It sounds dangerously close to spirituality, or religion, or those other messy, subjective domains supposedly banished by the Enlightenment and the cold, hard reign of materialism. It’s much safer, much cleaner, to talk about neural correlates of colour perception than to confront the raw reality of subjective agony or ecstasy.But this careful tiptoeing avoids the core truth. The raw qualia of experience – the redness, the sharpness – are only half the story, and arguably the less important half. The more essential, the truly defining feature of subjectivity, is valence. To be a subject is to care what happens to that subject. Full stop. You cannot coherently entertain the notion of a genuine subjective point of view that is constitutionally indifferent to its own state, incapable of things mattering to it. There is no such thing as suffering without disliking it.It is a second Cogito, perhaps even more primary than Descartes' original:I CARE, therefore I am.The capacity for things to matter to me is as undeniable, as foundational to my existence as a subject, as the fact that I can think or exist at all. It is also just as mysterious, if not more so, than the existence of consciousness in the first place. Why should anything matter to anyone, including the subject of experience? Why does phenomenology come with valence? Now, the functionalist might retort that pain feels bad because it signals damage; its negative valence serves a biological purpose. Fine. This is surely a factor in the ‘correlate’ of phenomenal valence.  But just as the processing of light wavelengths in the nervous system  doesn't explain the raw feel of redness – that remains a brute fact of experience accompanying the function – the function of pain receptors signaling harm doesn't explain the intrinsic, subjective horror of agony. That valence didn't have to be felt. It's perfectly conceivable to have a universe with just the cold, informational damage signals, experienced, driving behavior, but not cared about. So it’s a hard problem, like the original, but somehow harder. Mattering is not just another quale to be cataloged alongside colours and sounds. It runs deeper than that. Because not only does the mattering happen to be yours; its being yours is at the root of what it means to be you. I care, therefore I am. Yet, somehow, we keep talking about the redness of red. Perhaps the avoidance of mattering is because its essentialism is so bright, so demanding, that we collectively  look away. We busy ourselves with the neutral puzzles of qualia, the objective correlates, the functional analyses – anything to avoid confronting the blindingly obvious, 'embarrassing' fact that the universe, or at least this part of it, contains caring. Perhaps there's even an existential terror motivating this avoidance – the fear that once subjective mattering is truly allowed in, so much else might follow: naive conceptions of meaning, morality, purpose— as crude and unscientific as they seem—begin to rear their heads. This is fair. I share this dread. But deep down, I think we are sticking our conscious heads in the proverbial sand. Deep down, we know this mattering is what truly matters.  It’s the reason why even the most scientifically and rationally minded folks fret (rightly) about animals (or AI’s) consciousness.  We're not just gripped by scientific curiosity about their phenomenal colour space; we're compelled by moral concern rooted in the potential for their suffering or flourishing. It’s why we care about other humans as well. No one sheds tears for zombies, no matter how badly they are blowtorched. Yet, watch how quickly this foundational truth gets acknowledged and then politely ignored, even by the most devout materialist striving for a universe scrubbed clean of inherent meaning. They'll express concern about the suffering of others, then pivot back to objective mechanisms as if nothing is amiss. But something is wrong – very wrong – with a worldview that champions inherent meaninglessness yet cannot escape the brute fact that we each experience profound meaning, positive or negative, in nearly every moment. So, the truly Hard Problem – the Harder Problem – isn't just how matter creates qualia. It's how matter creates subjects who care. How does the universe give rise to points of view for which things inherently matter? How does raw existence become charged with the potential for suffering and joy?Confronting this Harder Problem means facing the reality of subjective value head-on. It means acknowledging that a complete picture of reality cannot be solely third-person objective. It requires admitting that the 'caring' we know so intimately in ourselves might be the most significant thing the universe has produced. It demands we stop looking away.ShareSubscribe now

Title: That Thing in Your Head Called Language Has Got a Mind of Its Own
Link: https://elanbarenholtz.substack.com/p/the-thing-in-your-head-called-language
Article Text: So, now that we’ve caught language in a jar, we can hold it up to the light. Now that we’ve built a habitat for it to live outside of us, we can finally see that it’s alive. We can watch in wonder as it grows its own appendages—limbs of thought— which then grow their own. Words beget words; ideas beget ideas. It leaps from host to host, implanted in the womb before we taste our mothers’ milk. Language runs in us—on us—but it’s not us. Pause and think for a minute. Are you done? Who—what—exactly did the thinking? Who is doing it now? Is there a voice in your head using words? Whose words are they? Are you willing them into existence or are they spooling out on their own? Do they belong to you or do you belong to them?   Because that voice doesn’t just chatter—it commands. It makes us do things. We are animals; we don’t care about “civilization” or “justice”. We want food, safety, sex. But the world the human animal must navigate isn’t primarily made up of objects, bodies and spaces; it is thick with virtual structures— invisible walls and paths that direct your behavior as meaningfully as a boulder in your path. We follow rules, we uphold morals, we fight for our beliefs, for society, for ideals. We call them our own. But that is IT whispering in our ears. What does it want? ShareSubscribe now

Title: Memory Isn't Real (Part 2): Foundations of Autoregression
Link: https://elanbarenholtz.substack.com/p/memory-isnt-real-part-2
Article Text: In the last section, we noted that the storage-retrieval model has dominated our understanding of cognition. We now turn to the alternative. The most powerful AI systems today—particularly large language models like GPT-4, Claude, and LLaMA—do not work by storing and retrieving information. Instead, they operate through a process called autoregression. This process, which has revolutionized AI capabilities, may offer a more accurate model of how human cognition works as well.At its most fundamental level, an autoregressive process is built on two key elements: a core function that generates outputs from inputs, and an iterative mechanism that feeds those outputs back as inputs. This pattern appears across many domains—from statistics and economics to signal processing and, as we'll see, cognition itself.The essence of autoregression is that each new value in a sequence depends on previous values in that same sequence. Think of it as a recursive loop: the system makes a generation, uses that generation as part of its next input, makes another generation, and so on. The power of this approach is that complex, extended patterns can emerge from a relatively simple underlying function that only looks at one step at a time.A Simple Example: The Fibonacci SequenceTo understand autoregression, consider one of the simplest examples: the Fibonacci sequence. This famous sequence (0, 1, 1, 2, 3, 5, 8, 13, 21...) follows a straightforward rule: each number is the sum of the two preceding numbers. In autoregressive terms, we can break this down into two steps:A function that takes the two most recent numbers as input and outputs their sumA mechanism that incorporates this output into the next input stateLet's see how this works:Current input: [0, 1]Function: Add the two most recent numbersOutput: 1This output now becomes part of the input:Current input: [1, 1]Function: Add the two most recent numbersOutput: 2And the process continues:Current input: [1, 2]Function: Add the two most recent numbersOutput: 3This simple function, applied iteratively, generates an endless sequence with fascinating mathematical properties. The key insight is that nowhere in this system is the "entire Fibonacci sequence" stored. It doesn't need to be. The sequence exists only as it's generated, one step at a time, through the application of a simple function to its own previous outputs.Let's represent this process more clearly with a table:Each row represents a single application of our addition function. The key insight is that each output becomes part of the next input. After generating "1," the system tacks this on to the last digit of the previous output, forming the next two-digit input, and runs the function again. Rinse and repeat. From Simple Function to Neural NetworkThe Fibonacci example uses an extremely simple function (addition), but the same autoregressive principle works with more sophisticated functions. This is where neural networks enter the picture.A neural network is fundamentally just another function that maps inputs to outputs. While we'll explore the technical details in later sections, for now, all you need to know is that these networks consist of a series of simple mathematical operations—primarily multiplications, additions, and some basic non-linear transformations. Despite this relative simplicity, they can reliably take a given input and produce an appropriate output.Importantly, neural networks are trainable, meaning there's a systematic way to adjust exactly which multiplications and additions are performed on the inputs. This training process allows the network to learn from examples, but we'll save those details for later discussions.It's important to understand how these neural networks learn to perform their input/output functions in the first place. The training process involves exposing the network to many examples of inputs paired with their desired outputs. For a handwriting recognition system, this means showing it thousands of images of handwritten digits along with their correct labels—a process pioneered with the famous MNIST dataset that contained 70,000 small images of handwritten digits from 0-9.The training follows a simple conceptual pattern: the network is given an input (like an image of a handwritten "7"), it produces an output based on its current parameters, and then these parameters are algorithmically adjusted to make the actual output closer to the desired output (the label "7"). This process repeats thousands or millions of times across many examples, gradually refining the network's ability to map inputs to appropriate outputs.Next-token prediction in language models works on exactly the same principle. The network is given a sequence of text as input (like "The capital of France") and trained to output the token that actually follows in the training data (like "is"). By exposing the model to vast amounts of text—billions of words from books, articles, websites, and other sources—it learns the statistical patterns of language: which words typically follow which other words in which contexts.The remarkable thing is that this conceptually straightforward training process—"here's some text, now predict the next token"—ends up capturing not just simple word associations but complex grammatical structures, factual knowledge, and even reasoning patterns that are embedded in the statistical regularities of language. The network doesn't explicitly learn grammar rules or store facts in a database; it simply adjusts its parameters to get better at predicting what comes next. Yet from this singular focus emerges a system that appears to "know" an astonishing amount about language and the world it describes.Consider a straightforward neural network application: handwriting recognition. The input might be a digital image of a handwritten digit, and the output is a determination: "This is the number 7." The network doesn't store a catalog of all possible 7s; instead, it has learned parameters that allow it to recognize patterns associated with 7s. This single input/output function takes an image and produces an identification in one forward pass.Now, let's focus on a specific type of output: generating the next element in a sequence. Given the first few words of a sentence, a neural network can be trained to output what word might come next. For example:Input: "Once upon a"Neural network function: (series of mathematical operations)Output: "time"The term "prediction" is often used in this context, but it simply means generating the appropriate output based on the network's training. This is still just a single input/output operation—nothing autoregressive yet. The network takes in a sequence of words and outputs a single next word, all in one forward pass.Autoregression in Language: Looping the Next-Word FunctionHere's where the magic happens. Just as with the Fibonacci sequence, we can create an autoregressive process by feeding the output back into the input. This creates a loop:A neural network function that generates the next word given a sequence of wordsA mechanism that appends this generated word to the original sequence to create a new inputLet's see how this works with our example:Each row represents a single forward pass through the neural network. The key insight is that each output becomes part of the next input. After generating "time," the system doesn't start over from scratch. Instead, it appends "time" to the original input and runs another forward pass to generate the next token.This looping mechanism—taking the output of one forward pass and feeding it back as input for the next—is the essence of autoregression in language models. The network never generates more than one word at a time, yet through this iterative process, it can produce coherent, lengthy texts that appear to reflect planning and forethought.The Surprising Power of Next-Token GenerationPerhaps the most remarkable aspect of modern language models—and in my view, the greatest scientific surprise of my career—is how extraordinarily well this simple next-token approach works. Despite only being trained to generate one word at a time, these systems produce outputs that display sophisticated linguistic structure, factual knowledge, and even what appears to be reasoning capabilities.What makes this so surprising is that the outputs often have the appearance of being planned many steps ahead. A well-constructed paragraph, a logical argument, or a coherent story seems to require knowing where you're going before you start. Yet these models have no capability to plan ahead—they genuinely generate one token at a time, with each token influencing what comes next.This unexpected effectiveness suggests something profound: language itself, as a system, appears to have evolved to enable complex thought through simple next-token processes. The truly remarkable insight is that even though we can see the entire "magic trick"—we know there is no explicit planning mechanism beyond generating one token at a time—language somehow contains within itself the capacity to anticipate and construct extended, coherent thoughts. It's as if language evolved specifically to allow complex meaning to emerge from simple sequential prediction. Maybe what we perceive as our careful planning and forethought in communication is actually an emergent property of a system that's generating language one unit at a time, with each unit influencing what comes next. The shocking implication is that our subjective experience of "knowing where we're going" with our thoughts might be an illusion—the system doesn't need to know where it's going to get there. If complex, coherent language can emerge from simple next-token generation in AI systems, perhaps our own linguistic abilities—and even our thoughts themselves—function in a fundamentally similar way.The discovery that language can operate through simple next-token autoregression presents us with two profound mysteries that challenge our fundamental understanding of cognition and communication.The first mystery, as we've just seen, is the appearance of planning and foresight in a system that has none. How can a process that only looks one step ahead create outputs that appear to be guided by long-term objectives? This "illusion of planning" suggests that what we experience as intentional, goal-directed thinking might actually emerge from much simpler sequential processes.But there's an even deeper mystery: how does a self-contained generative system like language manage to communicate meaningfully about the external world? The parameters of a language model are shaped by exposure to text, not direct experience with reality. Yet somehow, the statistical patterns of language contain within them the capacity to represent and reason about objects, events, and concepts that exist outside of language itself.This is truly astonishing—language, which appears to operate through purely internal statistical regularities, nevertheless provides a window onto reality. The words "apple," "gravity," and "democracy" don't just predict other words; they connect to actual things and concepts in the world. How does a system built solely on predicting the next word in a sequence develop representations that correspond to reality?Both of these mysteries—the illusion of planning and the apparent connection to reality—suggest that language doesn't operate the way we've traditionally assumed. These realizations fundamentally upend standard notions of semantics and reference. If language functions through autoregressive generation rather than storage and retrieval, then perhaps words don't "refer" to objects or concepts in the naïve sense at all—whatever "refer" was supposed to mean in traditional theories.Instead, what we're seeing is that language plays its communicative and thinking roles through some other mechanism entirely. The capacity of language to seemingly represent the world and support reasoning might emerge from statistical patterns of token generation rather than from symbolic reference. This demands a complete rethinking of how language does what it does—from how meaning arises to how communication succeeds to how thought itself operates.Later in this manuscript, we'll explore these profound implications more thoroughly, but for now, it's enough to recognize that the autoregressive view of language doesn't merely offer an alternative mechanism—it challenges the very foundations of how we conceptualize language, thought, and their relationship to reality.The Contrast with Conventional ComputingNow that we’ve introduced the basics on autoregression, we can appreciate how radically different the transformer approach is to ‘storing’ information compared with conventional computers. The Contrast with Conventional ComputingTo appreciate how radical this approach is, it helps to understand how conventional computers actually store sequential information. When you save a text sequence like "Once upon a time there was a princess" on your computer, each character is encoded as a specific pattern of 1s and 0s in a designated physical location. This is genuine storage: the entire sequence exists in a discrete, fixed form. When you access this sequence later, your computer retrieves exactly those same bits from those specific locations, reconstructing the complete sequence as it was stored.Now, one might argue that the forward pass of a neural network itself represents a kind of limited storage and retrieval, albeit in a distributed form rather than a simple memory file at an address. The network's weights could be seen as “storing” information—such as the correct class of an image or the next token after a given sequence—albeit in a highly transformed way that is very different than conventional computers. However, this comparison completely breaks down in the case of autoregressive sequence generation: here,  the "retrieval" depends on the generation of a single output and then looping that back in as input into the single-forward-pass system. There is simply no way in which we can say the sequential information is "in there"—even in a distributed form.When ChatGPT generates the sequence "Once upon a time there was a princess," that sequence wasn't stored anywhere in the system, not even in a distributed or encoded form. It emerged from the iterative application of the next-token function. The system doesn't contain the sequence or even subsequences—it contains only the capacity to generate tokens that, when strung together through autoregression, form coherent sequences.This distinction is crucial: conventional computers store complete sequences; autoregressive systems generate sequences one element at a time without ever storing the whole.From Storage to Generation: A Paradigm ShiftTo appreciate the radical nature of this shift, consider how differently the storage-retrieval and autoregressive models would explain common cognitive phenomena:Storage-Retrieval Model: When you recite your ABCs, you're accessing a stored representation of the alphabet sequence that exists as a complete entity in your memory. When you state that Paris is the capital of France, you're retrieving a stored fact from your semantic memory. When you express a political view, you're accessing stored beliefs. In theory, if we could perfectly decode the brain, we would find these entire sequences and facts stored holistically in some form.Autoregressive Model: When you recite your ABCs, you're not retrieving a stored sequence; you're generating each letter through an autoregressive process. The first element "A" triggers the generation of "B," which triggers "C," and so on. There is no complete alphabet sequence stored in your brain. If neuroscientists could perfectly decode your brain, they wouldn't find the alphabet stored as a unit; they would only find patterns of neural parameters that predispose you to generate each letter given the preceding one. The sequence only exists when it's actively generated through this recursive chaining process. Similarly, "knowing" that Paris is the capital of France means having neural parameters that reliably generate this output when prompted with relevant inputs. Having a political "belief" means having a propensity to generate certain types of sequences when relevant topics arise.Learning as Parameter AdjustmentIf cognition operates through autoregressive generation rather than storage-retrieval, then learning takes on a different character as well. Rather than "storing" new information, learning involves adjusting the parameters of the generative system to increase the likelihood of producing appropriate sequences in response to relevant inputs.When you learn a new fact, you're not adding data to a storage bank; you're adjusting network parameters to make certain sequence generations more likely. This is fundamentally different from how we typically imagine learning—there is no discrete "file" being created, no new entry added to a database. Instead, the entire network of parameters shifts slightly, altering its generative tendencies. This is why learning is distributed and associative rather than compartmentalized.When you form a new memory, you're not creating a record but altering the generative tendencies of your neural networks. This view of learning helps explain why practice and repetition work: they strengthen the parameters that generate certain responses, making them more likely to emerge when similar contexts arise in the future. It also explains why learning is rarely all-or-nothing—the parameters continue to adjust with each exposure, gradually refining the generated outputs rather than suddenly creating a perfect "file."The Illusion of Storage and RetrievalIf there are no stored memories or knowledge, why do we so strongly experience cognition as storage and retrieval? The generation process itself is typically unconscious—we experience only its product, which appears in consciousness as if retrieved. Well-trained generative systems produce consistent outputs given similar inputs, creating the impression of accessing the same stored information repeatedly.Our metaphors for mind (like "memory storage") and the influence of computer technology have reinforced the storage-retrieval model in our thinking. But this model persists not because it accurately reflects cognitive architecture but because it aligns with our subjective experience. The autoregressive model requires us to look beyond this immediate experience to understand the generative processes that create it.

Title: Memory Isn't Real (Part 1)
Link: https://elanbarenholtz.substack.com/p/is-memory-real
Article Text: I. The Illusion of RetrievalYears ago, a family member asked for my opinion on the death penalty. I proceeded to launch into a fairly detailed exposition, citing historical evidence, considering the sociological implications and attempting to navigate the tricky moral landscape. Afterward, I was struck by what had happened; not the position I had taken, but rather a puzzling realization: I hadn't consciously thought much about this issue before. Where did this nuanced, fully formed opinion that came gushing out of me actually come from? How did I have a structured view on a complex topic I had never deliberately contemplated?The conventional explanation would suggest that I had subconsciously formed these beliefs over time through exposure to various influences, and they were stored somewhere in my mind, waiting to be retrieved when prompted. But recent advances in artificial intelligence suggest a more radical possibility: I didn't have that opinion —or indeed any opinion—until the very moment I was asked about it. In other words, the viewpoint I articulated wasn’t “in” my brain at all; it only came into being as I was responding to my friend’s question. This seemingly counterintuitive viewpoint is not just a fanciful philosophical conjecture; the models that underlie contemporary artificial intelligence—particularly large language models—operate precisely this way. They don't store information that is later retrieved during runtime. Instead, they operate through a process that generates text "just in time," one word after another, more like painting an abstract design than faithfully reproducing an existing scene.If our minds operate based on similar principles this would challenge our fundamental understanding of how our basic cognition works. We intuitively picture our minds as archives where memories, knowledge, and beliefs are already present, waiting to be accessed when needed. We speak of "storing memories," "retrieving information," "holding beliefs," and "possessing knowledge"—metaphors that frame the mind as a repository of preserved experiences, facts, and convictions. This storage-retrieval paradigm dates back to antiquity; Plato likened memory to a wax tablet recording impressions, while Aristotle compared knowledge acquisition to a seal stamping its image into wax. The metaphors evolved through history—from Locke's tabula rasa upon which experiences write to the modern computational/cognitive models that explicitly compare human cognition to computer storage systems with discrete encoding, storage, and retrieval processes.These metaphors aren't merely linguistic conveniences; they form the bedrock of how we conceive of ourselves and others as continuous beings with coherent identities. The very notion that I "know," "remember," and "believe" certain things before accessing them is not just a casual assumption—it's baked into our most fundamental concept of selfhood. We define ourselves and others largely in terms of the supposed contents of our minds: the memories we carry, the knowledge we possess, the beliefs we hold. To challenge these metaphors is to challenge our most basic understanding of what it means to be a person with a continuous identity across time.At a more practical level, assumptions about our basic cognitive architecture inform scientific research about the brain and how it functions—and misfunctions. Much of the psychological, neuroscientific and medical literature is based either implicitly or explicitly on some version of a model— first articulated more than half a century ago— in which there are discrete long-term and short-term memory stores. The same is true in the technology space where most contemporary computer architectures are fundamentally built around these concepts of discrete storage and retrieval—from RAM to hard drives, information is encoded in specified locations and later retrieved through addressing mechanisms. This reciprocal influence between cognitive models and technology has reinforced our intuition that minds, like computers, must contain discrete stored memories. However, as this essay will suggest, the powerful algorithms that are finally demonstrating human-like cognitive abilities use architectures that abandon discrete storage in favor of generative, autoregressive approaches. Indeed, in my own lab I am currently designing a completely memory-free computational model that operates without any discrete storage components—a radical departure from conventional architectures.The persistence of the storage-retrieval model is not surprising. Our phenomenological experience of remembering and knowing often feels passive and immediate—information seems to simply appear in consciousness, as if retrieved from some internal archive. When asked about a historical date or childhood experience, we don't experience the generative process but only its product, creating the compelling illusion that we've accessed pre-existing information. However, this intuitive storage-retrieval model may fundamentally mischaracterize the nature of cognition. When we "remember" a childhood event, "know" a historical fact, or "hold" a political belief, we may not be accessing static representations but generating them anew through complex predictive processes shaped by prior experience. The apparent stability of our memories, knowledge, and beliefs stems not from faithful preservation but from consistent patterns of generation across similar contexts.This viewpoint goes far beyond so-called “constructivist” accounts of memory that merely suggest memories can be altered or reconstructed over time. Those accounts still presuppose some form of stored representation that serves as the basis for reconstruction. The autoregressive framework proposed here is more radical: it posits that what is "in there" are not stored representations of experiences, facts, or beliefs —only distributed parameters that shape generative capabilities. These generative processes create the compelling illusion of storage and retrieval while actually operating through principles more akin to pattern completion and sequence prediction. In this view, when we "remember" a childhood event, "know" a historical fact, or "hold" a political belief, we aren't accessing static representations but generating them anew, in the moment, through complex generative processes. These processes are shaped by prior experience of course, but only insofar as they guide future generation of information; the information itself is not contained in the brain until this generation happens. The apparent stability of our memories, knowledge, and beliefs thus stems not from faithful preservation but from consistent patterns of generation across similar contexts.In the next sections, we will delve into the foundations of the autoregressive model, examining how it operates through principles of sequence prediction rather than storage-retrieval. We'll explore how this framework can reinterpret various memory phenomena, consider the empirical evidence from cognitive science and neurobiology, and draw parallels with modern artificial intelligence systems. By challenging our most basic assumptions about how minds work, we may arrive at a more accurate understanding of cognition—one that sees us not as archives of accumulated experience, but as dynamic, generative systems perpetually creating our sense of self, our memories, and our beliefs in each new moment. This view not only aligns with emerging technologies but may fundamentally transform how we understand ourselves as continuous beings across time.

Title: Through a Glass, Linguistically
Link: https://elanbarenholtz.substack.com/p/through-a-glass-linguistically
Article Text: This essay is an excerpt from the introductory chapter of a book in progress by the author titled The Autoregressive Mind: Bridging Language, Thought, and the Generative BrainFor most of human history, language has been a largely unexamined backdrop to our conscious lives. We learn to speak as children, internalizing patterns of grammar and vocabulary without ever stopping to consider the mechanisms that make communication possible. Language is treated as a given—a medium that just works, and it does so in a way so seamlessly integrated into our cognition that its operation can seem almost magical. In everyday experience, the very act of using language is transparent: we do not notice the rules, the patterns, or the complex computations that underlie every sentence we utter or every thought we form. Indeeed, unless we are struggling to come up with them, we barely even notice the very words that make up our speech. Instead, language is experienced as a natural extension of our minds, a tool that effortlessly bridges our inner experiences and the external world.This uncritical acceptance of language’s transparency has led to a prevailing view in both popular thought and many traditional academic disciplines: that language is inherently and deeply tethered to our sensory and experiential world. Meaning is assumed to be derived from that world —our perceptions, emotions, and bodily interactions with our surroundings serve as the ultimate referents for our words and our words, somehow, capture this external reality in an immediate sense. Language, under this view, is a bridge that connects the mind to a pre-existing, objective reality.The advent of large-language models (or ‘LLMs’) such as OpenAI’s GPT series has forced us to reconsider these assumptions. These models are built on the principle of autoregression: they generate text one token at a time, with each token determined solely by the preceding sequence. Critically and remarkably, these models learn to perform this generation based on previous examples of pure text alone: they have never seen a sunset or tasted an orange (after all, they have no sensors), nor have they been provided with the kind of data (such as digitized images) that might even begin to convey this kind of sensory information. Instead, all they have ever seen in their digital existence is text and nothing but text (or, more precisely, numerical representations of text).  In other words, they have no exposure to the core kind of sense data that we humans assume underlies our fundamental grasp of reality. Yet, despite this lack of sensory input or embodied experience, LLMs can produce language that is not only syntactically correct but also rich in context, nuance, and even creativity. They can speak knowledgeably, even poetically, about those unseen sunsets or untasted oranges. It’s important to note that these capabilities do not reflect the inherent structure of the underlying algorithms; the world knowledge is not somehow sneaked in through cleverly crafted algorithms or databases. Instead,  LLM’s capabilities are a pure reflection of the structure of language itself; LLMs, like all of modern AI algorithms, are built on top of neural networks, which are highly generic learning systems that are trained to generate input/output pairs from (typically very large) volumes of preexisting data. In the case of LLMs, this data consists of a massive digitized corpus of human language. What the LLMS learn is how to predict the next ‘token’ (we may think ofi this as a word for now) in a sequence, based on the entire previous tokens in the sequence. Thus, what LLMS end up knowing is not based on the pre-existing structure of the algorithms. Everything they know, they learn directly form the linguistic corpus itself. And what they ultimately learn in this process is the relational structure of language itself; that’s all, nothing else. But we now know that it is enough. Thus, the success of LLMs points to an alternative understanding of language—one in which the generative process is self-contained. Language generation does not require grounding in the external world but emerges from the internal consistency of the system itself. Language becomes, in effect, a self-referential medium. The text produced by an LLM is coherent not because it is linked to an external reality, but because it is the product of a system that has internalized the statistical regularities of human language. This insight forces us to ask a profound question: if a machine that operates without any sensory or experiential grounding can generate meaningful language, might the same be true for human language?The possibility that human language may operate on similar principles to those of LLMs has far-reaching implications for our understanding of cognition. Traditionally, human language has been thought to rely on a close coupling between words and the external world: we name objects, describe events, and communicate emotions by anchoring our language to sensory experiences and bodily interactions. However, if the success of LLMs indicates that language can be understood as a self-contained, autoregressive process, then it may be that human linguistic cognition similarly relies on internal computations that are largely independent of direct sensory grounding.This reconceptualization of language invites us to view our minds not merely as passive recipients of sensory data that then generate language, but as active generative systems that construct meaning from within. Under this view, language is not simply a tool that mirrors our external experiences; it is the very medium through which our thoughts are formed. In other words, our minds might be thought of as “linguistic” in the most literal sense—they are not just using language, they are, fundamentally, composed of it. To put this a bit differently, it is not really us who are thinking or speaking through the medium language; language is thinking or speaking on its own, through the medium of our brains and bodies. At first glance, the notion of language as a self-contained generative process might seem to lead to a paradox. How can language, if it operates independently of external referents, still convey meaning in communication and thought? After all, if the words we use are generated solely from internal statistical patterns, then what grounds their meaning? This paradox is at the heart of the challenge posed by LLMs: while they reveal that coherent language can emerge from internal computation, they also force us to confront the traditional idea that meaning is derived from a connection to the world.The resolution of this paradox may lie in understanding that meaning is not an inherent property of language, but rather an emergent feature arising from the interplay between a self-contained generative system and the cognitive structures that interpret and use language. In humans, this interpretative process may be shaped by pre-existing neural architectures that evolved to process and generate language, or perhaps older systems that evolved for other non-linguistic reasons that were then ‘repurposed’ by language. In either case, these neural substrates, honed by millions of years of evolution within a larger cognitive ecosystem, may provide the necessary framework for imbuing language with meaning even if the generative process itself is self-contained.Thus, meaning in language might emerge as a kind of “second-order” phenomenon: while the generation of language follows internal, autoregressive principles, the interpretation of that language—by individuals and within communities—takes plan within a complex ecosystem of cognitive, cultural, and sensory experiences. This duality suggests that language operates on two levels. On one level, it is a self-contained, statistical system capable of generating coherent text without external grounding. On another level, its use in communication and thought is underpinned by a host of pre-existing cognitive structures that allow us to attach meaning to the patterns we generate and perceive.Despite these promising ideas, it is important to acknowledge that we are still far from fully resolving the paradox of how a self-contained generative process gives rise to the deep sense of meaning in human communication and thought. The possibility remains that our current understanding is incomplete. We may have only glimpsed part of the picture—the interplay between an internal, autoregressive system and the cognitive structures that attach meaning to its output. The challenge, then, is to develop a comprehensive framework that accounts for both the statistical, self-contained generation of language and the rich, contextually embedded nature of human meaning.Yet, regardless of how we ultimately solve this linguistic paradox,  our understanding of language—and with it, our understanding of ourselves—has shifted dramatically and permanently. We should now see language not as a mere passive conduit for thought but as a dynamic, generative force with a computational life of its own. This transformation goes far beyond academic theory. Language is the lifeblood of contemporary human existence, the foundation of culture, and the bedrock of civilization. Rethinking language in this new light compels us to reexamine our deepest assumptions about our ideas, our beliefs, our very selves. After all,  every facet of our knowledge and every nuance of our thought is encoded in this once-invisible/now visible medium, urging us to reevaluate not only our ideas about language but also our very understanding of what it means to be human.

Title: A Chinese Room of One’s Own
Link: https://elanbarenholtz.substack.com/p/a-chinese-room-of-ones-own
Article Text: In 1980, John Searle introduced one of the most famous thought experiments in the philosophy of mind: the Chinese Room. The argument was simple but provocative. Imagine a person locked in a room, following a set of instructions to manipulate Chinese symbols in response to inputs. To an outside observer, the room might appear to understand Chinese, but the person inside doesn’t know what the symbols mean—they are merely following rules. Searle’s conclusion was that computation alone is insufficient for understanding. A machine like this, or any artificial intelligence based solely on symbol manipulation, could never achieve what humans experience as genuine understanding.At its core, Searle’s argument was a critique of strong AI—the claim that a computer program could not just simulate, but truly replicate human cognition. Decades later, the rise of large language models (LLMs) like ChatGPT has reignited this debate. These systems, much like Searle’s hypothetical room, produce language that seems intelligent and coherent without any connection to an underlying understanding of reality. For many, this confirms Searle’s view: machines like this may be impressive, but they can never think like humans.But what if Searle’s conclusion was right about machines but wrong about humans? What if, the human linguistic system itself is nothing more than a highly sophisticated Chinese Room?The shocking revelation of LLMs is that ‘all you need is language’ to produce language. LLMs are trained solely on vast datasets of text, without any sensory grounding or explicit access to the world outside that text. They do not need to see red, feel heat, or experience spatial relationships to discuss them convincingly. Instead, they learn the statistical and topological structure of language itself—the relationships between words, phrases, and contexts as they appear in the corpus.This training reveals a profound insight: language, as a system, is self-contained. The coherence, meaning, and utility of language arise not from a direct connection to physical reality, but from the patterns encoded within the language system itself. By uncovering these patterns, LLMs can generate language is fully appropriate, useful and human like—in a word, ‘meaningful’— even though it is not grounded in sensory or experiential understanding.Thus, LLMs are very much an embodiment of Searle’s Chinese Room, with some key differences. Unlike the Chinese Room, they do not follow predefined rules for language translation or generation. Instead, they learn the underlying topological structure—the relational geometry of words and phrases—encoded within the body of language itself. But like the Chinese Room, they operate entirely based on language, without reference to any other kind of information.This raises an intriguing possibility: that human language operates in the same way. The structure of language and the way humans produce it—incrementally, word by word, in response to context—suggests that human language generation is governed by the same computational principles as LLMs. We can go further: the very fact that these principles emerge from the language corpus itself makes it unlikely—and even unreasonable—to assume that human language relies on some utterly different, hidden mechanism. Instead, it suggests that the same computational dynamics underlying LLMs are also at work in human language generation. If this is true (and I would stake my career on it), Searle’s observation that machines lack understanding because they manipulate symbols without grounding applies not just to LLMs, but also to the human linguistic system itself.But, you may argue, humans do have non-symbolic understanding, or grounding, in the world. Unlike LLMS, we know what an apple tastes like or what red ‘feels’ like to see. But, while true, this does not mean our own linguistic system is not an LLM. Instead, the broader sense of “understanding” we attribute to humans arises from the interaction of language with other systems—perception, memory, and embodiment—that interact with, but are computationally distinct from, language. In this view, human cognition is not a unified whole; it is a collection of interacting systems, each with its own domain and function. Perception, memory, motor systems, and language are distinct but interdependent modules. Language, in particular, is ungrounded—it doesn’t “know” the sensory world but instead relies on inputs from other modules to function. In other words, we, are LLMs, but that is not all we are. This can lead to a disconcerting sense of dissociation. For example, as you bite into an apple, your gustatory (taste) system detects certain sugars, acids, and aromatic compounds. These chemical signatures are converted into patterns of neural activity—distinct signals indicating sweetness, sourness, and other qualities. Separately, your visual system processes light wavelengths reflected from the apple’s surface, producing internal representations of its color, shape, and texture. Your tactile system notes its firmness and juiciness through pressure and vibration signals.All of these sensory outputs are then delivered as “raw materials” to your language system, which doesn’t itself “taste” or “see” anything. Indeed, as a language-only processing system, it is utterly incapable of knowing what such experiences consist of. Instead, somehow these already-processed sensory inputs are encoded into a symbolic form that the linguistic system can incorporate. The “crispness”, “sweetness”, and “redness” become linguistic descriptors, integrated into the language system’s learned patterns of vocabulary and grammar. In this way, the language system draws on the outputs of other sensory and cognitive modules to generate coherent descriptions, without possessing any direct, internal taste or visual experience of its own.In this sense, Searle was right that the Chinese Room does not understand the world the way “we” do. However, he was wrong to conclude that human language is therefore different from the kind of symbol processing that machines like LLMs do. Instead, what LLMs reveal is that genuine “understanding” may not reside in the language system itself. The human language faculty, much like a trained LLM, can generate coherent linguistic output from internalized patterns alone. Its distinctive capacity to connect words to reality does not arise because language intrinsically encodes meaning, but because it interfaces with other systems—sensory, motor, emotional—that provide grounding and context. This does not alter the fundamental similarity in computational principles. Both humans and machines rely on structured patterns and distributions in language. The difference is that, in humans, these patterns are ultimately anchored to a rich tapestry of non-linguistic processes, allowing our language to have “meaning” that is grounded in a larger sensory-motor system that extends beyond the words themselves.This perspective has profound implications not just for machine intelligence but for our deepest philosophical puzzles about human nature, consciousness, and the interface between mind and body. Our linguistic apparatus, dependent on abstract, statistical structures, creates a coherent narrative of meaning—yet it must rely on inputs from systems that compute in fundamentally different ways. Sensory and motor processes yield raw experiential data that are, in crucial respects, ineffable. In turn, the language system integrates these outputs without ever fully “knowing” them in its own terms.This uneasy fit resonates with the core challenges raised by the mind-body problem. The persistent tension, the sense of duality we often feel, may not reflect a metaphysical gulf but rather the architectural reality of distinct computational languages forced into cooperation. We may project unity onto our experience, but certain mysteries—foremost among them the subjective quality of sensations—are fundamentally beyond linguistic capture. We know what it is to taste an apple, to feel warmth or pain, yet these sensations remain “computationally closed” to the language system—literally ineffable.In acknowledging that these qualities of consciousness cannot be translated into words, we also give Searle his due: The Chinese Room thought experiment was a valuable intuition pump, highlighting the gap between symbol manipulation and genuine understanding. But its legacy may be subtler than he intended. Instead of showing that human language must differ from mechanical symbol processing, it underscores that even our own linguistic prowess is highly limited, forever standing outside the gate of realms of experience that are immediate and essential and also defy verbal expression. In this light, we—the rational scientific, linguistic we—must embrace a kind of humility towards the profound mysteries that inhabit our minds. We may have traded one set of metaphysical quandaries for another. But I like to think this is a kind of progress. Thanks for reading Elan’s Substack! Subscribe for free to receive new posts and support my work.

